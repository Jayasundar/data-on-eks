"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[107],{3905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>k});var n=a(7294);function l(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function s(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){l(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,l=function(e,t){if(null==e)return{};var a,n,l={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(l[a]=e[a]);return l}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(l[a]=e[a])}return l}var i=n.createContext({}),p=function(e){var t=n.useContext(i),a=t;return e&&(a="function"==typeof e?e(t):s(s({},t),e)),a},u=function(e){var t=p(e.components);return n.createElement(i.Provider,{value:t},e.children)},c="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,l=e.mdxType,r=e.originalType,i=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),c=p(a),d=l,k=c["".concat(i,".").concat(d)]||c[d]||m[d]||r;return a?n.createElement(k,s(s({ref:t},u),{},{components:a})):n.createElement(k,s({ref:t},u))}));function k(e,t){var a=arguments,l=t&&t.mdxType;if("string"==typeof e||l){var r=a.length,s=new Array(r);s[0]=d;var o={};for(var i in t)hasOwnProperty.call(t,i)&&(o[i]=t[i]);o.originalType=e,o[c]="string"==typeof e?e:l,s[1]=o;for(var p=2;p<r;p++)s[p]=a[p];return n.createElement.apply(null,s)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},5162:(e,t,a)=>{a.d(t,{Z:()=>s});var n=a(7294),l=a(6010);const r={tabItem:"tabItem_Ymn6"};function s(e){let{children:t,hidden:a,className:s}=e;return n.createElement("div",{role:"tabpanel",className:(0,l.Z)(r.tabItem,s),hidden:a},t)}},4866:(e,t,a)=>{a.d(t,{Z:()=>w});var n=a(7462),l=a(7294),r=a(6010),s=a(2466),o=a(6550),i=a(1980),p=a(7392),u=a(12);function c(e){return function(e){return l.Children.map(e,(e=>{if(!e||(0,l.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:t,label:a,attributes:n,default:l}}=e;return{value:t,label:a,attributes:n,default:l}}))}function m(e){const{values:t,children:a}=e;return(0,l.useMemo)((()=>{const e=t??c(a);return function(e){const t=(0,p.l)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,a])}function d(e){let{value:t,tabValues:a}=e;return a.some((e=>e.value===t))}function k(e){let{queryString:t=!1,groupId:a}=e;const n=(0,o.k6)(),r=function(e){let{queryString:t=!1,groupId:a}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:t,groupId:a});return[(0,i._X)(r),(0,l.useCallback)((e=>{if(!r)return;const t=new URLSearchParams(n.location.search);t.set(r,e),n.replace({...n.location,search:t.toString()})}),[r,n])]}function h(e){const{defaultValue:t,queryString:a=!1,groupId:n}=e,r=m(e),[s,o]=(0,l.useState)((()=>function(e){let{defaultValue:t,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!d({value:t,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const n=a.find((e=>e.default))??a[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:t,tabValues:r}))),[i,p]=k({queryString:a,groupId:n}),[c,h]=function(e){let{groupId:t}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(t),[n,r]=(0,u.Nk)(a);return[n,(0,l.useCallback)((e=>{a&&r.set(e)}),[a,r])]}({groupId:n}),g=(()=>{const e=i??c;return d({value:e,tabValues:r})?e:null})();(0,l.useLayoutEffect)((()=>{g&&o(g)}),[g]);return{selectedValue:s,selectValue:(0,l.useCallback)((e=>{if(!d({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);o(e),p(e),h(e)}),[p,h,r]),tabValues:r}}var g=a(2389);const b={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function f(e){let{className:t,block:a,selectedValue:o,selectValue:i,tabValues:p}=e;const u=[],{blockElementScrollPositionUntilNextRender:c}=(0,s.o5)(),m=e=>{const t=e.currentTarget,a=u.indexOf(t),n=p[a].value;n!==o&&(c(t),i(n))},d=e=>{let t=null;switch(e.key){case"Enter":m(e);break;case"ArrowRight":{const a=u.indexOf(e.currentTarget)+1;t=u[a]??u[0];break}case"ArrowLeft":{const a=u.indexOf(e.currentTarget)-1;t=u[a]??u[u.length-1];break}}t?.focus()};return l.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.Z)("tabs",{"tabs--block":a},t)},p.map((e=>{let{value:t,label:a,attributes:s}=e;return l.createElement("li",(0,n.Z)({role:"tab",tabIndex:o===t?0:-1,"aria-selected":o===t,key:t,ref:e=>u.push(e),onKeyDown:d,onClick:m},s,{className:(0,r.Z)("tabs__item",b.tabItem,s?.className,{"tabs__item--active":o===t})}),a??t)})))}function y(e){let{lazy:t,children:a,selectedValue:n}=e;const r=(Array.isArray(a)?a:[a]).filter(Boolean);if(t){const e=r.find((e=>e.props.value===n));return e?(0,l.cloneElement)(e,{className:"margin-top--md"}):null}return l.createElement("div",{className:"margin-top--md"},r.map(((e,t)=>(0,l.cloneElement)(e,{key:t,hidden:e.props.value!==n}))))}function N(e){const t=h(e);return l.createElement("div",{className:(0,r.Z)("tabs-container",b.tabList)},l.createElement(f,(0,n.Z)({},e,t)),l.createElement(y,(0,n.Z)({},e,t)))}function w(e){const t=(0,g.Z)();return l.createElement(N,(0,n.Z)({key:String(t)},e))}},769:(e,t,a)=>{a.d(t,{Z:()=>d});var n=a(7294),l=a(5697),r=a.n(l),s=a(6010);const o="collapsibleContent_q3kw",i="header_QCEw",p="icon_PckA",u="content_qLC1",c="expanded_iGsi";function m(e){let{children:t,header:a}=e;const[l,r]=(0,n.useState)(!1);return n.createElement("div",{className:o},n.createElement("div",{className:(0,s.Z)(i,{[c]:l}),onClick:()=>{r(!l)}},a,n.createElement("span",{className:(0,s.Z)(p,{[c]:l})})),l&&n.createElement("div",{className:u},t))}m.propTypes={children:r().node.isRequired,header:r().node.isRequired};const d=m},6739:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>p,default:()=>h,frontMatter:()=>i,metadata:()=>u,toc:()=>m});var n=a(7462),l=(a(7294),a(3905)),r=a(4866),s=a(5162),o=a(769);const i={title:"Flink Operator on EKS",sidebar_position:3},p=void 0,u={unversionedId:"blueprints/streaming-platforms/flink",id:"blueprints/streaming-platforms/flink",title:"Flink Operator on EKS",description:"Please note that we are working on adding more features to this blueprint such as Flink examples with multiple connectors, Ingress for WebUI, Grafana dashboards etc.",source:"@site/docs/blueprints/streaming-platforms/flink.md",sourceDirName:"blueprints/streaming-platforms",slug:"/blueprints/streaming-platforms/flink",permalink:"/data-on-eks/docs/blueprints/streaming-platforms/flink",draft:!1,editUrl:"https://github.com/awslabs/data-on-eks/blob/main/website/docs/blueprints/streaming-platforms/flink.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{title:"Flink Operator on EKS",sidebar_position:3},sidebar:"blueprints",previous:{title:"EMR on EKS with Spark Streaming",permalink:"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-stream"},next:{title:"Kafka on EKS",permalink:"/data-on-eks/docs/blueprints/streaming-platforms/kafka"}},c={},m=[{value:"Introduction to Apache Flink",id:"introduction-to-apache-flink",level:2},{value:"Architecture",id:"architecture",level:2},{value:"Flink Kubernetes Operator",id:"flink-kubernetes-operator",level:2},{value:"Best Practices for Running Flink Jobs on Kubernetes",id:"best-practices-for-running-flink-jobs-on-kubernetes",level:2},{value:"Flink Upgrade",id:"flink-upgrade",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Deploy",id:"deploy",level:3},{value:"Autoscaling Job",id:"autoscaling-job",level:4},{value:"Execute Sample Flink Autoscaling job with Karpenter",id:"execute-sample-flink-autoscaling-job-with-karpenter",level:3},{value:"Execute Sample Flink Autoscaling job with Managed Node Groups and Cluster Autoscaler",id:"execute-sample-flink-autoscaling-job-with-managed-node-groups-and-cluster-autoscaler",level:3}],d={toc:m},k="wrapper";function h(e){let{components:t,...i}=e;return(0,l.kt)(k,(0,n.Z)({},d,i,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("admonition",{type:"info"},(0,l.kt)("p",{parentName:"admonition"},"Please note that we are working on adding more features to this blueprint such as Flink examples with multiple connectors, Ingress for WebUI, Grafana dashboards etc.")),(0,l.kt)("h2",{id:"introduction-to-apache-flink"},"Introduction to Apache Flink"),(0,l.kt)("p",null,(0,l.kt)("a",{parentName:"p",href:"https://flink.apache.org/"},"Apache Flink")," is an open-source, unified stream processing and batch processing framework that was designed to process large amounts of data. It provides fast, reliable, and scalable data processing with fault tolerance and exactly-once semantics.\nSome of the key features of Flink are:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Distributed Processing"),": Flink is designed to process large volumes of data in a distributed fashion, making it horizontally scalable and fault-tolerant."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Stream Processing and Batch Processing"),": Flink provides APIs for both stream processing and batch processing. This means you can process data in real-time, as it's being generated, or process data in batches."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Fault Tolerance"),": Flink has built-in mechanisms for handling node failures, network partitions, and other types of failures."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Exactly-once Semantics"),": Flink supports exactly-once processing, which ensures that each record is processed exactly once, even in the presence of failures."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Low Latency"),": Flink's streaming engine is optimized for low-latency processing, making it suitable for use cases that require real-time processing of data."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Extensibility"),": Flink provides a rich set of APIs and libraries, making it easy to extend and customize to fit your specific use case."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Observability")," Flink exposes a metrics system that gathers and exposes metrics to the external systems such as Prometheus. The Flink Operator extends this to centralized monitoring solutions."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Autoscaling")," Flink supports Reactive and Adaptive autoscaling. Job autoscaling is supported by the Flink operator for resource and cost optimizations.")),(0,l.kt)("h2",{id:"architecture"},"Architecture"),(0,l.kt)("p",null,"Flink Architecture high level design with EKS."),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"Flink Design UI",src:a(7697).Z,width:"5078",height:"2207"})),(0,l.kt)("h2",{id:"flink-kubernetes-operator"},"Flink Kubernetes Operator"),(0,l.kt)("p",null,(0,l.kt)("a",{parentName:"p",href:"https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/"},"Flink Kubernetes Operator")," is a powerful tool for managing Flink clusters on Kubernetes. Flink Kubernetes Operator (Operator) acts as a control plane to manage the complete deployment lifecycle of Apache Flink applications. The Operator can be installed on a Kubernetes cluster using Helm. The core responsibility of the Flink operator is to manage the full production lifecycle of Flink applications."),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"Running, suspending and deleting applications"),(0,l.kt)("li",{parentName:"ol"},"Stateful and stateless application upgrades"),(0,l.kt)("li",{parentName:"ol"},"Triggering and managing savepoints"),(0,l.kt)("li",{parentName:"ol"},"Handling errors, rolling-back broken upgrades")),(0,l.kt)("p",null,"Flink Operator defines two types of Custom Resources(CR) which are the extensions of the Kubernetes API."),(0,l.kt)(r.Z,{mdxType:"Tabs"},(0,l.kt)(s.Z,{value:"FlinkDeployment",label:"FlinkDeployment",mdxType:"TabItem"},(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"FlinkDeployment")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"FlinkDeployment CR defines ",(0,l.kt)("strong",{parentName:"p"},"Flink Application")," and ",(0,l.kt)("strong",{parentName:"p"},"Session Cluster")," deployments.")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"Application deployments manage a single job deployment on a dedicated Flink cluster in Application mode.")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"Session clusters allows you to run multiple Flink Jobs on an existing Session cluster."),(0,l.kt)("details",null,(0,l.kt)("summary",null,"FlinkDeployment in Application modes, Click to toggle content!"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: flink.apache.org/v1beta1\nkind: FlinkDeployment\nmetadata:\nnamespace: default\nname: basic-example\nspec:\nimage: flink:1.16\nflinkVersion: v1_16\nflinkConfiguration:\n    taskmanager.numberOfTaskSlots: "2"\nserviceAccount: flink\njobManager:\n    resource:\n    memory: "2048m"\n    cpu: 1\ntaskManager:\n    resource:\n    memory: "2048m"\n    cpu: 1\njob:\n    jarURI: local:///opt/flink/examples/streaming/StateMachineExample.jar\n    parallelism: 2\n    upgradeMode: stateless\n    state: running\n')))))),(0,l.kt)(s.Z,{value:"FlinkSessionJob",label:"FlinkSessionJob",mdxType:"TabItem"},(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"FlinkSessionJob")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"The ",(0,l.kt)("inlineCode",{parentName:"p"},"FlinkSessionJob")," CR defines the session job on the ",(0,l.kt)("strong",{parentName:"p"},"Session cluster")," and each Session cluster can run multiple ",(0,l.kt)("inlineCode",{parentName:"p"},"FlinkSessionJob"),".")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"Session deployments manage Flink Session clusters without providing any job management for it"),(0,l.kt)("details",null,(0,l.kt)("summary",null,'FlinkSessionJob using an existing "basic-session-cluster" session cluster deployment'),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: flink.apache.org/v1beta1\nkind: FlinkSessionJob\nmetadata:\nname: basic-session-job-example\nspec:\ndeploymentName: basic-session-cluster\njob:\n    jarURI: https://repo1.maven.org/maven2/org/apache/flink/flink-examples-streaming_2.12/1.15.3/flink-examples-streaming_2.12-1.15.3-TopSpeedWindowing.jar\n    parallelism: 4\n    upgradeMode: stateless\n"))))))),(0,l.kt)("admonition",{type:"info"},(0,l.kt)("p",{parentName:"admonition"},"Session clusters use a similar spec to Application clusters with the only difference that ",(0,l.kt)("inlineCode",{parentName:"p"},"job")," is not defined in the yaml spec.")),(0,l.kt)("admonition",{type:"info"},(0,l.kt)("p",{parentName:"admonition"},"According to the Flink documentation, it is recommended to use FlinkDeployment in Application mode for production environments.")),(0,l.kt)("p",null,"On top of the deployment types the Flink Kubernetes Operator also supports two modes of deployments: ",(0,l.kt)("inlineCode",{parentName:"p"},"Native")," and ",(0,l.kt)("inlineCode",{parentName:"p"},"Standalone"),"."),(0,l.kt)(r.Z,{mdxType:"Tabs"},(0,l.kt)(s.Z,{value:"Native",label:"Native",mdxType:"TabItem"},(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Native")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Native cluster deployment is the default deployment mode and uses Flink\u2019s built in integration with Kubernetes when deploying the cluster."),(0,l.kt)("li",{parentName:"ul"},"Flink cluster communicates directly with Kubernetes and allows it to manage Kubernetes resources, e.g. dynamically allocate and de-allocate TaskManager pods."),(0,l.kt)("li",{parentName:"ul"},"Flink Native can be useful for advanced users who want to build their own cluster management system or integrate with existing management systems."),(0,l.kt)("li",{parentName:"ul"},"Flink Native allows for more flexibility in terms of job scheduling and execution."),(0,l.kt)("li",{parentName:"ul"},"For standard Operator use running your own Flink Jobs Native mode is recommended")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: flink.apache.org/v1beta1\nkind: FlinkDeployment\n...\nspec:\n...\nmode: native\n"))),(0,l.kt)(s.Z,{value:"Standalone",label:"Standalone",mdxType:"TabItem"},(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Standalone")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"Standalone cluster deployment simply uses Kubernetes as an orchestration platform that the Flink cluster is running on.")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"Flink is unaware that it is running on Kubernetes and therefore all Kubernetes resources need to be managed externally, by the Kubernetes Operator."),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: flink.apache.org/v1beta1\nkind: FlinkDeployment\n...\nspec:\n...\nmode: standalone\n")))))),(0,l.kt)("h2",{id:"best-practices-for-running-flink-jobs-on-kubernetes"},"Best Practices for Running Flink Jobs on Kubernetes"),(0,l.kt)("p",null,"To get the most out of Flink on Kubernetes, here are some best practices to follow:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Use the Kubernetes Operator"),": Install and use the Flink Kubernetes Operator to automate the deployment and management of Flink clusters on Kubernetes."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Deploy in dedicated namespaces"),": Create a separate namespace for the Flink Kubernetes Operator and another one for Flink jobs/workloads. This ensures that the Flink jobs are isolated and have their own resources."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Use high-quality storage"),": Store Flink checkpoints and savepoints in high-quality storage such as Amazon S3 or another durable external storage. These storage options are reliable, scalable, and offer durability for large volumes of data."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Optimize resource allocation"),": Allocate sufficient resources to Flink jobs to ensure optimal performance. This can be done by setting resource requests and limits for Flink containers."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Proper network isolation"),": Use Kubernetes Network Policies to isolate Flink jobs from other workloads running on the same Kubernetes cluster. This ensures that Flink jobs have the required network access without being impacted by other workloads."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Configure Flink optimally"),": Tune Flink settings according to your use case. For example, adjust Flink's parallelism settings to ensure that Flink jobs are scaled appropriately based on the size of the input data."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Use checkpoints and savepoints"),": Use checkpoints for periodic snapshots of Flink application state and savepoints for more advanced use cases such as upgrading or downgrading the application."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Store checkpoints and savepoints in the right places"),": Store checkpoints in distributed file systems or key-value stores like Amazon S3 or another durable external storage. Store savepoints in a durable external storage like Amazon S3."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Cost Optimization"),": Flink Task Managers can be scheduled on discounted spare EC2 capacity (EC2 Spot Instances) with minimal impact to SLA.")),(0,l.kt)("h2",{id:"flink-upgrade"},"Flink Upgrade"),(0,l.kt)("p",null,"Flink Operator provides three upgrade modes for Flink jobs. Checkout the ",(0,l.kt)("a",{parentName:"p",href:"https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/custom-resource/job-management/#stateful-and-stateless-application-upgrades"},"Flink upgrade docs")," for up-to-date information."),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("strong",{parentName:"li"},"stateless"),": Stateless application upgrades from empty state"),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("strong",{parentName:"li"},"last-state"),": Quick upgrades in any application state (even for failing jobs), does not require a healthy job as it always uses the latest checkpoint information. Manual recovery may be necessary if HA metadata is lost."),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("strong",{parentName:"li"},"savepoint"),": Use savepoint for upgrade, providing maximal safety and possibility to serve as backup/fork point. The savepoint will be created during the upgrade process. Note that the Flink job needs to be running to allow the savepoint to get created. If the job is in an unhealthy state, the last checkpoint will be used (unless kubernetes.operator.job.upgrade.last-state-fallback.enabled is set to false). If the last checkpoint is not available, the job upgrade will fail.")),(0,l.kt)("admonition",{type:"info"},(0,l.kt)("p",{parentName:"admonition"},(0,l.kt)("inlineCode",{parentName:"p"},"last-state")," or ",(0,l.kt)("inlineCode",{parentName:"p"},"savepoint")," are recommended modes for production")),(0,l.kt)(o.Z,{header:(0,l.kt)("h2",null,(0,l.kt)("span",null,"Deploying the Solution")),mdxType:"CollapsibleContent"},(0,l.kt)("p",null,"In this ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/awslabs/data-on-eks/tree/main/streaming/flink"},"example"),", you will provision the following resources required to run Flink Jobs with Flink Operator and Apache YuniKorn."),(0,l.kt)("p",null,"This example deploys an EKS Cluster running the Flink Operator into a new VPC."),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Creates a new sample VPC, 2 Private Subnets and 2 Public Subnets"),(0,l.kt)("li",{parentName:"ul"},"Creates Internet gateway for Public Subnets and NAT Gateway for Private Subnets"),(0,l.kt)("li",{parentName:"ul"},"Creates EKS Cluster Control plane with public endpoint (for demo reasons only) with core managed node group, on-demand node group and Spot node group for Flink workloads."),(0,l.kt)("li",{parentName:"ul"},"Deploys Metrics server, Cluster Autoscaler, Apache YuniKorn, Karpenter, Grafana, AMP and Prometheus server."),(0,l.kt)("li",{parentName:"ul"},"Deploys Cert Manager and Flink Operator add-ons. Flink Operator has dependency on Cert Manager"),(0,l.kt)("li",{parentName:"ul"},"Creates a new Flink Data team resources that includes namespace, service account, IRSA, Role and Role binding."),(0,l.kt)("li",{parentName:"ul"},"Deploys three Karpenter provisioners for different compute types")),(0,l.kt)("h3",{id:"prerequisites"},"Prerequisites"),(0,l.kt)("p",null,"Ensure that you have installed the following tools on your machine."),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html"},"aws cli")),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("a",{parentName:"li",href:"https://Kubernetes.io/docs/tasks/tools/"},"kubectl")),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("a",{parentName:"li",href:"https://learn.hashicorp.com/tutorials/terraform/install-cli"},"terraform"))),(0,l.kt)("h3",{id:"deploy"},"Deploy"),(0,l.kt)("p",null,"Clone the repository"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"git clone https://github.com/awslabs/data-on-eks.git\n")),(0,l.kt)("p",null,"Navigate into one of the example directories and run ",(0,l.kt)("inlineCode",{parentName:"p"},"install.sh")," script"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"cd data-on-eks/streaming/flink\nchmod +x install.sh\n./install.sh\n")),(0,l.kt)("p",null,"Verify the cluster status"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"\u279c kubectl get nodes -A\nNAME                                         STATUS   ROLES    AGE   VERSION\nip-10-1-160-150.us-west-2.compute.internal   Ready    <none>   24h   v1.24.11-eks-a59e1f0\nip-10-1-169-249.us-west-2.compute.internal   Ready    <none>   6d    v1.24.11-eks-a59e1f0\nip-10-1-69-244.us-west-2.compute.internal    Ready    <none>   6d    v1.24.11-eks-a59e1f0\n\n\u279c  ~ kubectl get pods -n flink-kubernetes-operator\nNAME                                         READY   STATUS    RESTARTS   AGE\nflink-kubernetes-operator-77697fb949-rwqqm   2/2     Running   0          24h\n\n\u279c  ~ kubectl get pods -n cert-manager\nNAME                                      READY   STATUS    RESTARTS   AGE\ncert-manager-77fc7548dc-dzdms             1/1     Running   0          24h\ncert-manager-cainjector-8869b7ff7-4w754   1/1     Running   0          24h\ncert-manager-webhook-586ddf8589-g6s87     1/1     Running   0          24h\n")),(0,l.kt)("p",null,"To list all the resources created for Flink team to run Flink jobs using this namespace"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"\u279c  ~ kubectl get all,role,rolebinding,serviceaccount --namespace flink-team-a-ns\nNAME                                               CREATED AT\nrole.rbac.authorization.k8s.io/flink-team-a-role   2023-04-06T13:17:05Z\n\nNAME                                                              ROLE                     AGE\nrolebinding.rbac.authorization.k8s.io/flink-team-a-role-binding   Role/flink-team-a-role   22h\n\nNAME                             SECRETS   AGE\nserviceaccount/default           0         22h\nserviceaccount/flink-team-a-sa   0         22h\n"))),(0,l.kt)(o.Z,{header:(0,l.kt)("h2",null,(0,l.kt)("span",null,"Execute Sample Flink job with Karpenter")),mdxType:"CollapsibleContent"},(0,l.kt)("p",null,"Navigate to example directory and submit the Flink job."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"cd data-on-eks/streaming/flink/examples/karpenter\nkubectl apply -f flink-sample-job.yaml\n")),(0,l.kt)("p",null,"Monitor the job status using the below command.\nYou should see the new nodes triggered by the karpenter and the YuniKorn will schedule one Job manager pod and one Taskmanager pods on this node."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get deployments -n flink-team-a-ns\nNAME            READY   UP-TO-DATE   AVAILABLE   AGE\nbasic-example   1/1     1            1           5m9s\n\nkubectl get pods -n flink-team-a-ns\nNAME                            READY   STATUS    RESTARTS   AGE\nbasic-example-bf467dff7-zwhgc   1/1     Running   0          102s\nbasic-example-taskmanager-1-1   1/1     Running   0          87s\nbasic-example-taskmanager-1-2   1/1     Running   0          87s\n\nkubectl get services -n flink-team-a-ns\nNAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE\nbasic-example-rest   ClusterIP   172.20.74.9   <none>        8081/TCP   3m43s\n")),(0,l.kt)("p",null,"To access the Flink WebUI for the job run this command locally."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl port-forward svc/basic-example-rest 8081 -n flink-team-a-ns\n")),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"Flink Job UI",src:a(2760).Z,width:"3022",height:"1604"}),"\n",(0,l.kt)("img",{alt:"Flink Job UI",src:a(8454).Z,width:"3022",height:"1604"}),"\n",(0,l.kt)("img",{alt:"Flink Job UI",src:a(6877).Z,width:"3022",height:"1604"}),"\n",(0,l.kt)("img",{alt:"Flink Job UI",src:a(6534).Z,width:"3022",height:"1604"}),"\n",(0,l.kt)("img",{alt:"Flink Job UI",src:a(298).Z,width:"3022",height:"1604"}))),(0,l.kt)(o.Z,{header:(0,l.kt)("h2",null,(0,l.kt)("span",null,"Execute Sample Flink job with Managed Node Groups and Cluster Autoscaler")),mdxType:"CollapsibleContent"},(0,l.kt)("p",null,"Navigate to example directory and submit the Spark job."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"cd data-on-eks/streaming/flink/examples/cluster-autoscaler\nkubectl apply -f flink-sample-job.yaml\n")),(0,l.kt)("p",null,"Monitor the job status using the below command.\nYou should see the new nodes triggered by the Cluster Autoscaler and the YuniKorn will schedule one Job manager pod and one Taskmanager pods on this node."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get deployments -n flink-team-a-ns\nNAME            READY   UP-TO-DATE   AVAILABLE   AGE\nbasic-example   1/1     1            1           5m9s\n\nkubectl get pods -n flink-team-a-ns\nNAME                            READY   STATUS    RESTARTS   AGE\nbasic-example-bf467dff7-zwhgc   1/1     Running   0          102s\nbasic-example-taskmanager-1-1   1/1     Running   0          87s\nbasic-example-taskmanager-1-2   1/1     Running   0          87s\n\nkubectl get services -n flink-team-a-ns\nNAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE\nbasic-example-rest   ClusterIP   172.20.74.9   <none>        8081/TCP   3m43s\n")),(0,l.kt)("p",null,"To access the Flink WebUI for the job run this command locally."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl port-forward svc/basic-example-rest 8081 -n flink-team-a-ns\n")),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"Flink Job UI",src:a(2760).Z,width:"3022",height:"1604"}),"\n",(0,l.kt)("img",{alt:"Flink Job UI",src:a(8454).Z,width:"3022",height:"1604"}),"\n",(0,l.kt)("img",{alt:"Flink Job UI",src:a(6877).Z,width:"3022",height:"1604"}),"\n",(0,l.kt)("img",{alt:"Flink Job UI",src:a(6534).Z,width:"3022",height:"1604"}),"\n",(0,l.kt)("img",{alt:"Flink Job UI",src:a(298).Z,width:"3022",height:"1604"}))),(0,l.kt)(o.Z,{header:(0,l.kt)("h2",null,(0,l.kt)("span",null,"Observability")),mdxType:"CollapsibleContent"},(0,l.kt)("p",null,"In order to scrape the metrics to Prometheus and Grafana, Flink needs podMonitors. We are using two podMonitors here one for the Operator and one for deployments."),(0,l.kt)("h3",null,(0,l.kt)("span",null,"Install podMonitors ")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl apply -f https://raw.githubusercontent.com/awslabs/data-on-eks/main/streaming/flink/observability/flink-deployment-podMonitor.yaml\nkubectl apply -f https://raw.githubusercontent.com/awslabs/data-on-eks/main/streaming/flink/observability/flink-operator-podMonitor.yaml\n")),(0,l.kt)("h3",null,(0,l.kt)("span",null,"Prometheus and Grafana")),(0,l.kt)("p",null,"Prometheus and Grafana can be accessed through port forwarding."),(0,l.kt)("p",null,"Prometheus"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl port-forward prometheus-kube-prometheus-stack-prometheus-0 -n kube-prometheus-stack 9090:9090\n")),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"Flink Design UI",src:a(4389).Z,width:"2436",height:"1458"})),(0,l.kt)("p",null,"Grafana"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl port-forward svc/kube-prometheus-stack-grafana 8080:80 -n kube-prometheus-stack\n")),(0,l.kt)("p",null,"For grafana login and password use the following commands."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'kubectl get secret kube-prometheus-stack-grafana -n kube-prometheus-stack -o jsonpath="{.data.admin-user}" | base64 --decode ; echo\nkubectl get secret kube-prometheus-stack-grafana -n kube-prometheus-stack -o jsonpath="{.data.admin-password}" | base64 --decode ; echo\n'))),(0,l.kt)(o.Z,{header:(0,l.kt)("h2",null,(0,l.kt)("span",null,"Autoscaling")),mdxType:"CollapsibleContent"},(0,l.kt)("p",null,"We are using the Flink Autoscaler more details ",(0,l.kt)("a",{parentName:"p",href:"https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-release-1.5/docs/custom-resource/autoscaler/"},"here"),". A few key pointers"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Autoscaler with Flink 1.17 and beyond we need to backport for older versions."),(0,l.kt)("li",{parentName:"ul"},"Autocaler takes into account multiple metrics and scales Flink's ",(0,l.kt)("a",{parentName:"li",href:"https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/internals/job_scheduling/#jobmanager-data-structures"},"job vertexes"),"."),(0,l.kt)("li",{parentName:"ul"},"Flink Autoscaler works with Karpenter and Cluster Autoscaler(CA).",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"Flink schedules the TaskManager replicas(pods) by changing the replica factor and Karpenter/CA scales the nodes.")))),(0,l.kt)("h4",{id:"autoscaling-job"},"Autoscaling Job"),(0,l.kt)("p",null,"We are going to use the sample ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/apache/flink-kubernetes-operator/tree/main/examples/autoscaling"},"autoscaling")," code provided by Flink. We need to build a custom docker image, which we will push to Amazon ECR."),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"Build the jar file ","[Note: We need Java 11/8 and maven >3.8 ]"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"git clone git@github.com:apache/flink-kubernetes-operator.git\ncd flink-kubernetes-operator/examples/autoscaling\nmvn clean install -DskipTests\ndocker build -t flink-autoscaling .\n"))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"Login to ECR, create the repository and push the image. Replace ",(0,l.kt)("inlineCode",{parentName:"p"},"<Account_Id>")," with your AWS account ID."),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin <Account_Id>.dkr.ecr.us-west-2.amazonaws.com\naws ecr create-repository --repository-name flink-autoscaling\ndocker tag flink-autoscaling:latest <Account_Id>.dkr.ecr.us-west-2.amazonaws.com/flink-autoscaling:latest\ndocker push <Account_Id>.dkr.ecr.us-west-2.amazonaws.com/flink-autoscaling:latest\n")))),(0,l.kt)("h3",{id:"execute-sample-flink-autoscaling-job-with-karpenter"},"Execute Sample Flink Autoscaling job with Karpenter"),(0,l.kt)("p",null,"Navigate to examples/karpenter directory and submit the Flink job. Replace ",(0,l.kt)("inlineCode",{parentName:"p"},"<Account_Id>")," with your AWS account ID in the yaml file."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"cd data-on-eks/streaming/flink/examples/karpenter\nkubectl apply -f flink-autoscaling-job.yaml\n")),(0,l.kt)("p",null,"Monitor the job status using the below command. You should see the taskmanager pods triggered by the Flink and new nodes by Karpenter as in the screenshot below. It generally takes 3-5 mins for the autoscaling to trigger. You will also see Flink jobmanager getting restarted as Flink does not support in-place autoscaling yet."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"}," kubectl get pods -n flink-team-a-ns -w\n")),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"Flink Pods",src:a(4558).Z,width:"1178",height:"1158"})),(0,l.kt)("p",null,"We modified the argument values (Number of Iterations) in the example and saw different scaling trajectories which was noted in Grafana dashboard."),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"Flink Autoscaling 10 Iterations",src:a(2397).Z,width:"767",height:"338"})," ",(0,l.kt)("img",{alt:"Flink Autoscaling 1000 Iterations",src:a(3766).Z,width:"768",height:"309"}),(0,l.kt)("img",{alt:"Flink Autoscaling 100000 Iterations",src:a(9166).Z,width:"767",height:"315"})),(0,l.kt)("h3",{id:"execute-sample-flink-autoscaling-job-with-managed-node-groups-and-cluster-autoscaler"},"Execute Sample Flink Autoscaling job with Managed Node Groups and Cluster Autoscaler"),(0,l.kt)("p",null,"Navigate to examples/cluster-autoscaler directory and submit the Flink job. Replace ",(0,l.kt)("inlineCode",{parentName:"p"},"<Account_Id>")," with your AWS account ID in the yaml file."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"cd data-on-eks/streaming/flink/examples/cluster-autoscaler\nkubectl apply -f flink-autoscaling-job.yaml\n")),(0,l.kt)("p",null,"You can expect similar results as showcased in the Karpenter example above here too.")),(0,l.kt)(o.Z,{header:(0,l.kt)("h2",null,(0,l.kt)("span",null,"Execute Sample Beam Python Job on Flink")),mdxType:"CollapsibleContent"},(0,l.kt)("p",null,"We will be running a simple wordcount example with Beam pipelines using ",(0,l.kt)("a",{parentName:"p",href:"https://beam.apache.org/documentation/runners/flink/"},"Flink Runner")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"Build the docker image using the dockerfile provided in flink/examples/python. For more details take a look ",(0,l.kt)("a",{parentName:"p",href:"https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/docker/#using-flink-python-on-docker"},"here")),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"docker build -t flink-operator-python .\n"))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"Login to ECR, create the repository and push the image. Replace ",(0,l.kt)("inlineCode",{parentName:"p"},"<Account_Id>")," with your AWS account ID."),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin <Account_Id>.dkr.ecr.us-west-2.amazonaws.com\naws ecr create-repository --repository-name flink-operator-python\ndocker tag flink-operator-python:latest <Account_Id>.dkr.ecr.us-west-2.amazonaws.com/flink-operator-python:latest\ndocker push <Account_Id>.dkr.ecr.us-west-2.amazonaws.com/flink-operator-python:latest\n"))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"We would be deploying a Flink Session cluster which launches beam-worker-pools as sidecars. Replace ",(0,l.kt)("inlineCode",{parentName:"p"},"<Account_Id>")," with your AWS account ID in the yaml file."),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-bash"}," kubectl apply -f flink-operator-python-beam-session-cluster.yaml\n"))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"Once the session cluster is deployed we will launch the wordcount job using the command below"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-bash"}," kubectl apply -f flink-operator-python-beam-job.yaml\n"))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"You can observe the job/cluster details as the screenshot below by port-forwarding and launching locahost:8081"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-bash"}," kubectl port-forward <Session Cluster Pod> 8081 -n flink-team-a-ns\n")))),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"Flink Job UI",src:a(1498).Z,width:"1891",height:"837"}))),(0,l.kt)(o.Z,{header:(0,l.kt)("h2",null,(0,l.kt)("span",null,"Cleanup")),mdxType:"CollapsibleContent"},(0,l.kt)("p",null,"This script will cleanup the environment using ",(0,l.kt)("inlineCode",{parentName:"p"},"-target")," option to ensure all the resources are deleted in correct order."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"cd data-on-eks/streaming/flink && chmod +x cleanup.sh\n./cleanup.sh\n"))),(0,l.kt)("admonition",{type:"caution"},(0,l.kt)("p",{parentName:"admonition"},"To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment")))}h.isMDXComponent=!0},2397:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/flink-autoscaling-10-d35a63c679572f20bd09628d60cfe65b.png"},3766:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/flink-autoscaling-1000-5ca660ec47130acf37dee4d66340274c.png"},9166:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/flink-autoscaling-100000-5b7e3652adbc7b06383b1a7617f24aea.png"},4558:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/flink-autoscaling-03b83c050556015971095bb08ddf7064.png"},1498:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/flink-beam-b29181b293555c1b0839e48f8c16264b.png"},7697:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/flink-design-76636a1cd4f5b26cb4c136a805d8edb9.png"},4389:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/flink-prometheus-02cdc73e47bc0004e5550bc8e6cff722.png"},2760:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/flink1-917ff73ed2e0467f8f5c3c4c3150bb85.png"},8454:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/flink2-61b9b18791139dd0b8412509e14a1d4a.png"},6877:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/flink3-ce2609f46e015076be7e99da7c953e55.png"},6534:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/flink4-191396cf259ff9c97125466de6b84a9a.png"},298:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/flink5-d8e8a986eab75fe25b338df4f9a0d031.png"}}]);